import numpy as np
from typing import Dict, Any

class AnalysisGenerator:
    """Classe para gerar insights autom√°ticos em linguagem natural"""
    
    def __init__(self):
        pass
    
    def generate_model_insights(self, results: Dict[str, Any]) -> Dict[str, str]:
        """Gera insights para cada modelo"""
        insights = {}
        
        for model_name, metrics in results.items():
            if 'error' in metrics:
                insights[model_name] = f"‚ùå Erro ao treinar o modelo {model_name}: {metrics['error']}"
                continue
            
            problem_type = metrics.get('problem_type', 'classification')
            insight_parts = []
            
            if problem_type == 'classification':
                # An√°lise da acur√°cia
                accuracy = metrics.get('accuracy', 0)
                if accuracy >= 0.9:
                    insight_parts.append(f"‚úÖ Excelente acur√°cia ({accuracy:.1%})")
                elif accuracy >= 0.8:
                    insight_parts.append(f"‚úÖ Boa acur√°cia ({accuracy:.1%})")
                elif accuracy >= 0.7:
                    insight_parts.append(f"‚ö†Ô∏è Acur√°cia moderada ({accuracy:.1%})")
                else:
                    insight_parts.append(f"‚ùå Acur√°cia baixa ({accuracy:.1%})")
                
                # An√°lise da precis√£o
                precision = metrics.get('precision', 0)
                if precision < 0.7:
                    insight_parts.append("‚ö†Ô∏è Precis√£o baixa - modelo gera muitos falsos positivos")
                elif precision >= 0.9:
                    insight_parts.append("‚úÖ Excelente precis√£o")
                
                # An√°lise do recall
                recall = metrics.get('recall', 0)
                if recall < 0.7:
                    insight_parts.append("‚ö†Ô∏è Recall baixo - modelo perde muitos casos positivos")
                elif recall >= 0.9:
                    insight_parts.append("‚úÖ Excelente recall")
                
                # An√°lise do F1-score
                f1 = metrics.get('f1', 0)
                if f1 >= 0.9:
                    insight_parts.append("‚úÖ F1-score excelente - bom balanceamento")
                elif f1 < 0.7:
                    insight_parts.append("‚ö†Ô∏è F1-score baixo - desbalanceamento entre precis√£o e recall")
                
                # ROC-AUC se dispon√≠vel
                roc_auc = metrics.get('roc_auc')
                if roc_auc is not None and not np.isnan(roc_auc):
                    if roc_auc >= 0.9:
                        insight_parts.append(f"‚úÖ ROC-AUC excelente ({roc_auc:.3f})")
                    elif roc_auc < 0.7:
                        insight_parts.append(f"‚ö†Ô∏è ROC-AUC baixo ({roc_auc:.3f})")
                        
            else:  # regression
                # An√°lise do R¬≤ Score
                r2 = metrics.get('r2_score', 0)
                if r2 >= 0.9:
                    insight_parts.append(f"‚úÖ Excelente R¬≤ ({r2:.3f}) - modelo explica muito bem a vari√¢ncia")
                elif r2 >= 0.8:
                    insight_parts.append(f"‚úÖ Bom R¬≤ ({r2:.3f}) - modelo explica bem a vari√¢ncia")
                elif r2 >= 0.6:
                    insight_parts.append(f"‚ö†Ô∏è R¬≤ moderado ({r2:.3f}) - modelo explica parte da vari√¢ncia")
                elif r2 >= 0:
                    insight_parts.append(f"‚ùå R¬≤ baixo ({r2:.3f}) - modelo explica pouco da vari√¢ncia")
                else:
                    insight_parts.append(f"‚ùå R¬≤ negativo ({r2:.3f}) - modelo pior que a m√©dia")
                
                # An√°lise do RMSE
                rmse = metrics.get('rmse', 0)
                y_mean = metrics.get('y_test_mean', 1)
                rmse_relative = rmse / y_mean if y_mean != 0 else float('inf')
                
                if rmse_relative < 0.1:
                    insight_parts.append(f"‚úÖ RMSE baixo ({rmse:.3f}) - predi√ß√µes muito precisas")
                elif rmse_relative < 0.2:
                    insight_parts.append(f"‚úÖ RMSE bom ({rmse:.3f}) - predi√ß√µes precisas")
                elif rmse_relative < 0.5:
                    insight_parts.append(f"‚ö†Ô∏è RMSE moderado ({rmse:.3f}) - predi√ß√µes razo√°veis")
                else:
                    insight_parts.append(f"‚ùå RMSE alto ({rmse:.3f}) - predi√ß√µes imprecisas")
                
                # An√°lise do MAE
                mae = metrics.get('mae', 0)
                mae_relative = mae / y_mean if y_mean != 0 else float('inf')
                
                if mae_relative < 0.1:
                    insight_parts.append("‚úÖ Erro m√©dio baixo")
                elif mae_relative > 0.3:
                    insight_parts.append("‚ö†Ô∏è Erro m√©dio alto")
            
            insights[model_name] = " | ".join(insight_parts)
        
        return insights
    
    def generate_comparison_insights(self, results: Dict[str, Any]) -> str:
        """Gera insights comparativos entre modelos"""
        valid_results = {k: v for k, v in results.items() if 'error' not in v}
        
        if not valid_results:
            return "‚ùå Nenhum modelo foi treinado com sucesso."
        
        if len(valid_results) == 1:
            model_name = list(valid_results.keys())[0]
            return f"‚úÖ Apenas o modelo {model_name} foi treinado com sucesso."
        
        # Detecta tipo de problema
        problem_type = list(valid_results.values())[0].get('problem_type', 'classification')
        
        insights = []
        
        if problem_type == 'classification':
            # Encontra o melhor modelo por F1-score
            best_model = max(valid_results.items(), key=lambda x: x[1].get('f1', 0))
            best_name, best_metrics = best_model
            
            insights.append(f"üèÜ Melhor modelo: {best_name} (F1-score: {best_metrics.get('f1', 0):.3f})")
            
            # Compara m√©tricas
            accuracies = {k: v.get('accuracy', 0) for k, v in valid_results.items()}
            highest_accuracy = max(accuracies.items(), key=lambda x: x[1])
            
            if highest_accuracy[0] != best_name:
                insights.append(f"üìä Maior acur√°cia: {highest_accuracy[0]} ({highest_accuracy[1]:.3f})")
            
            # An√°lise geral
            avg_accuracy = np.mean(list(accuracies.values()))
            if avg_accuracy >= 0.85:
                insights.append("‚úÖ Todos os modelos apresentaram boa performance geral")
            elif avg_accuracy < 0.7:
                insights.append("‚ö†Ô∏è Performance geral baixa - considere revisar os dados")
                
        else:  # regression
            # Encontra o melhor modelo por R¬≤ score
            best_model = max(valid_results.items(), key=lambda x: x[1].get('r2_score', -float('inf')))
            best_name, best_metrics = best_model
            
            insights.append(f"üèÜ Melhor modelo: {best_name} (R¬≤: {best_metrics.get('r2_score', 0):.3f})")
            
            # Compara RMSE (menor √© melhor)
            rmse_values = {k: v.get('rmse', float('inf')) for k, v in valid_results.items()}
            lowest_rmse = min(rmse_values.items(), key=lambda x: x[1])
            
            if lowest_rmse[0] != best_name:
                insights.append(f"üìâ Menor RMSE: {lowest_rmse[0]} ({lowest_rmse[1]:.3f})")
            
            # An√°lise geral baseada em R¬≤
            r2_values = [v.get('r2_score', 0) for v in valid_results.values()]
            avg_r2 = np.mean(r2_values)
            
            if avg_r2 >= 0.8:
                insights.append("‚úÖ Todos os modelos apresentaram boa capacidade explicativa")
            elif avg_r2 < 0.5:
                insights.append("‚ö†Ô∏è Capacidade explicativa baixa - considere revisar features")
        
        return " | ".join(insights)
    
    def generate_recommendations(self, results: Dict[str, Any], data_info: Dict[str, Any]) -> list:
        """Gera recomenda√ß√µes de melhoria"""
        recommendations = []
        
        valid_results = {k: v for k, v in results.items() if 'error' not in v}
        
        if not valid_results:
            return ["üîß Revisar qualidade dos dados - nenhum modelo foi treinado com sucesso"]
        
        # An√°lise do dataset
        rows = data_info.get('rows', 0)
        if rows < 1000:
            recommendations.append(f"üìà Dataset pequeno ({rows} linhas) - coletar mais dados pode melhorar a performance")
        
        features = data_info.get('features', 0)
        if features < 3:
            recommendations.append("üîß Poucas features - considere feature engineering para criar novas vari√°veis")
        
        # An√°lise da performance
        avg_f1 = np.mean([v.get('f1', 0) for v in valid_results.values()])
        
        if avg_f1 < 0.8:
            recommendations.extend([
                "üéØ Otimizar hiperpar√¢metros com GridSearchCV ou RandomizedSearchCV",
                "üîÑ Experimentar t√©cnicas de cross-validation",
                "üß™ Testar outros algoritmos (XGBoost, LightGBM, Neural Networks)"
            ])
        
        if avg_f1 < 0.7:
            recommendations.extend([
                "üîç An√°lise explorat√≥ria dos dados mais detalhada",
                "üßπ Verificar outliers e ru√≠dos nos dados",
                "‚öñÔ∏è Verificar balanceamento das classes"
            ])
        
        # Recomenda√ß√µes espec√≠ficas por tipo de problema
        n_classes = data_info.get('target_classes', 2)
        if n_classes > 2:
            recommendations.append("üìä Para problemas multiclasse, considere estrat√©gias one-vs-rest ou ensemble methods")
        
        if not recommendations:
            recommendations.append("‚úÖ Modelos com boa performance - considere deploy em produ√ß√£o")
        
        return recommendations
    
    def generate_full_analysis(self, pipeline_result: Dict[str, Any]) -> Dict[str, Any]:
        """Gera an√°lise completa dos resultados"""
        if not pipeline_result.get('success'):
            return {
                'success': False,
                'error': pipeline_result.get('error', 'Erro desconhecido'),
                'analysis': 'N√£o foi poss√≠vel gerar an√°lise devido a erro no pipeline.'
            }
        
        results = pipeline_result.get('results', {})
        data_info = pipeline_result.get('data_info', {})
        
        # Gera insights
        model_insights = self.generate_model_insights(results)
        comparison = self.generate_comparison_insights(results)
        recommendations = self.generate_recommendations(results, data_info)
        
        # Resumo executivo
        valid_models = len([r for r in results.values() if 'error' not in r])
        total_models = len(results)
        
        executive_summary = []
        executive_summary.append(f"üìã {valid_models}/{total_models} modelos treinados com sucesso")
        executive_summary.append(f"üìä Dataset: {data_info.get('rows', 0)} linhas, {data_info.get('features', 0)} features")
        executive_summary.append(f"üéØ {data_info.get('target_classes', 0)} classes para predi√ß√£o")
        
        return {
            'success': True,
            'executive_summary': executive_summary,
            'model_insights': model_insights,
            'comparison': comparison,
            'recommendations': recommendations,
            'data_info': data_info,
            'timestamp': pipeline_result.get('timestamp')
        }